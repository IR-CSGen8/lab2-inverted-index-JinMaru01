{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81d8d80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Document ID                                            Content\n",
      "0            1  {\\r\\n   \"title\": \"Introduction to Python\",\\r\\n...\n",
      "1            2  {\\r\\n   \"title\": \"Data Analysis with Pandas\",\\...\n",
      "2            3  {\\r\\n   \"title\": \"Web Development with Flask\",...\n",
      "3            4  {\\r\\n   \"title\": \"Machine Learning with Scikit...\n",
      "4            5  {\\r\\n   \"title\": \"Data Visualization with Matp...\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV dataset\n",
    "df = pd.read_csv(\"semi_strut.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a5ea1871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Document ID                                            Content\n",
      "0            1  {\\r\\n   \"title\": \"Introduction to Python\",\\r\\n...\n",
      "1            2  {\\r\\n   \"title\": \"Data Analysis with Pandas\",\\...\n",
      "2            3  {\\r\\n   \"title\": \"Web Development with Flask\",...\n",
      "3            4  {\\r\\n   \"title\": \"Machine Learning with Scikit...\n",
      "4            5  {\\r\\n   \"title\": \"Data Visualization with Matp...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenization function to extract terms from the JSON-like content\n",
    "# df['Content']\n",
    "list_of_content = [list(row) for row in df.values]\n",
    "# print(list_of_content[0])\n",
    "contents = []\n",
    "for a in list_of_content:\n",
    "    contents.append(a[1])\n",
    "#     print(a[1])\n",
    "obj = []\n",
    "\n",
    "for b in range(len(contents)):\n",
    "    obj.append(json.loads(contents[b]))\n",
    "\n",
    "def clean_word (word):\n",
    "    word = word.split('...')\n",
    "    return word[0]\n",
    "\n",
    "    \n",
    "def tokenize_content(content):\n",
    "    terms = []\n",
    "    terms.extend(content.get(\"title\").split())\n",
    "    terms.extend(content.get(\"author\").split())        \n",
    "    \n",
    "    section = content.get(\"sections\")\n",
    "    for a in range(len(section)):\n",
    "        list_word = []\n",
    "        terms.extend(section[a].get(\"title\", \"\").split())\n",
    "        for word in section[a].get(\"content\", \"\").split():\n",
    "            cleaned_word = clean_word(word)\n",
    "            list_word.append(cleaned_word)\n",
    "        terms.extend(list_word)\n",
    "\n",
    "    return terms\n",
    "\n",
    "def preprocess_terms(terms):\n",
    "    # Define a set of common stop words\n",
    "    stop_words = set([\n",
    "        \"a\", \"an\", \"the\", \"and\", \"is\", \"in\", \"it\", \"to\", \"of\", \"for\", \"on\", \"with\", \"as\"\n",
    "    ])\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    terms = [term.lower().strip(string.punctuation) for term in terms]\n",
    "    \n",
    "    # Remove stop words\n",
    "    terms = [term for term in terms if term not in stop_words]\n",
    "    \n",
    "    return terms\n",
    "\n",
    "unique_terms = set()\n",
    "def get_unique_terms(content):\n",
    "    unique_terms = {term for doc in content for term in doc.split()}\n",
    "    return unique_terms\n",
    "\n",
    "# for a in range(len(unique_terms)):\n",
    "#     print(unique_terms[0])\n",
    "\n",
    "list_of_term = []\n",
    "\n",
    "for a in range(len(obj)):\n",
    "    list_of_term.append(tokenize_content(obj[a]))\n",
    "    \n",
    "list_of_unique_terms = []\n",
    "\n",
    "for a in range(len(list_of_term)):\n",
    "#     print(list_of_term[a])\n",
    "    list_of_unique_terms.append(get_unique_terms(list_of_term[a]))\n",
    "# print(list_of_term[0])\n",
    "# print()\n",
    "# print(list_of_unique_terms[0])\n",
    "\n",
    "print(df.head())\n",
    "# unique_terms_list\n",
    "\n",
    "# Remember to exact both \n",
    "# 1 .Extract terms from various fields (title, keywords, and content)\n",
    "# 2. Extract terms from sections' titles and content\n",
    "# 3. Tokenize the content and create a new column \"Terms\"\n",
    "# 4. Implement a preprocessing function that converts terms to lowercase, removes punctuation, and removes common stop words.\n",
    "    # Create another new column \"Terms_preprocessed\"\n",
    "# 5. you can display the DataFrame\n",
    "# def tokenize_content(content):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27def6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131fdcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty inverted index dictionary\n",
    "# Build the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4824b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty inverted index dictionary the extracted terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b929397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform boolean operations on postings lists for Boolean search operations\n",
    "# 1. \"Python\" OR \"Pandas\"\n",
    "# 2. \"Python\" AND \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80680485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
