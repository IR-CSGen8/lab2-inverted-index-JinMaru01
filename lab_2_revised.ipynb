{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba64eec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Index:\n",
      "{'introduction': {1, 2, 4, 5}, 'python': {1, 2, 3, 4, 5}, 'john': {1}, 'doe': {1}, 'programming': {1}, 'beginner': {1}, 'getting': {1, 3}, 'started': {1, 3}, 'versatile': {1, 5}, 'language': {1}, 'basic': {1}, 'syntax': {1}, 'easy': {1}, 'understand': {1}, 'data': {2, 5}, 'analysis': {2}, 'pandas': {2}, 'jane': {2}, 'smith': {2}, 'data analysis': {2}, 'popular': {2}, 'library': {2, 5}, 'dataframes': {2}, 'are': {2}, 'core': {2}, 'structure': {2}, 'web': {3}, 'development': {3}, 'flask': {3}, 'mike': {3}, 'johnson': {3}, 'web development': {3}, 'lightweight': {3}, 'framework': {3}, 'routing': {3}, 'defines': {3}, 'url': {3}, 'patterns': {3}, 'views': {3}, 'machine': {4}, 'learning': {4}, 'scikit-learn': {4}, 'emily': {4}, 'davis': {4}, 'machine learning': {4}, 'subfield': {4}, 'artificial': {4}, 'intelligence': {4}, 'supervised': {4}, 'type': {4}, 'visualization': {5}, 'matplotlib': {5}, 'robert': {5}, 'clark': {5}, 'data visualization': {5}, 'creating': {5}, 'plots': {5}, 'can': {5}, 'be': {5}, 'created': {5}, 'using': {5}, 'various': {5}, 'functions': {5}}\n",
      "Documents matching 'Python' OR 'Pandas': {1, 2, 3, 4, 5}\n",
      "Documents matching 'Python' AND 'Data': {2, 5}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "\n",
    "# Load the CSV dataset\n",
    "df = pd.read_csv(\"semi_strut.csv\")\n",
    "\n",
    "# Tokenization function to extract terms from the JSON-like content\n",
    "def tokenize_content(content):\n",
    "    content_dict = json.loads(content)\n",
    "    terms = []\n",
    "    \n",
    "    # Extract terms from various fields (title, author)\n",
    "    terms.extend(content_dict.get(\"title\", \"\").split())\n",
    "    terms.extend(content_dict.get(\"author\", \"\").split())\n",
    "    # Extract keywords\n",
    "    keywords = content_dict.get(\"keywords\", [])\n",
    "    terms.extend(keywords)\n",
    "    \n",
    "    # Extract terms from sections' titles and content\n",
    "    sections = content_dict.get(\"sections\", [])\n",
    "    for section in sections:\n",
    "        terms.extend(section.get(\"title\", \"\").split())\n",
    "        terms.extend(section.get(\"content\", \"\").split())\n",
    "    \n",
    "    return terms\n",
    "\n",
    "# Apply the tokenization function to all rows in the DataFrame and create a new column \"Terms\"\n",
    "df[\"Terms\"] = df[\"Content\"].apply(tokenize_content)\n",
    "\n",
    "# Preprocess the terms and create another new column \"Terms_preprocessed\"\n",
    "def preprocess_terms(terms):\n",
    "    # Define a set of common stop words\n",
    "    stop_words = set([\n",
    "        \"a\", \"an\", \"the\", \"and\", \"is\", \"in\", \"it\", \"to\", \"of\", \"for\", \"on\", \"with\", \"as\"\n",
    "    ])\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    terms = [term.lower().strip(string.punctuation) for term in terms]\n",
    "    \n",
    "    # Remove stop words\n",
    "    terms = [term for term in terms if term not in stop_words]\n",
    "    \n",
    "    return terms\n",
    "\n",
    "df[\"Terms_preprocessed\"] = df[\"Terms\"].apply(preprocess_terms)\n",
    "\n",
    "# Initialize an empty inverted index dictionary\n",
    "inverted_index = {}\n",
    "\n",
    "# Build the inverted index\n",
    "for index, row in df.iterrows():\n",
    "    document_id = row[\"Document ID\"]\n",
    "    terms = row[\"Terms_preprocessed\"]\n",
    "    \n",
    "    # Update the inverted index with terms and document IDs\n",
    "    for term in terms:\n",
    "        if term not in inverted_index:\n",
    "            inverted_index[term] = set()\n",
    "        inverted_index[term].add(document_id)\n",
    "\n",
    "# Display the inverted index\n",
    "print(\"Inverted Index:\")\n",
    "print(inverted_index)\n",
    "\n",
    "# Perform Boolean operations on postings lists for Boolean search operations\n",
    "\n",
    "# 1. \"Python\" OR \"Pandas\"\n",
    "query1 = [\"python\", \"pandas\"]\n",
    "result1 = set()\n",
    "\n",
    "for term in query1:\n",
    "    if term in inverted_index:\n",
    "        result1.update(inverted_index[term])\n",
    "\n",
    "print(\"Documents matching 'Python' OR 'Pandas':\", result1)\n",
    "\n",
    "# 2. \"Python\" AND \"data\"\n",
    "query2 = [\"python\", \"data\"]\n",
    "result2 = set(df[\"Document ID\"])  # Initialize with all document IDs\n",
    "\n",
    "for term in query2:\n",
    "    if term in inverted_index:\n",
    "        result2.intersection_update(inverted_index[term])\n",
    "\n",
    "print(\"Documents matching 'Python' AND 'Data':\", result2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
